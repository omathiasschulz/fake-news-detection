+++ TÓPICOS QUE AINDA SERÃO COMPLEMENTADOS

4.3.1 Funções de Ativação
(TODO)
Texto quase pronto - Falta explicar as funções de ativação que serão utilizadas

Começar com a função ReLU e depois passar para outras funções de ativação no caso da ReLU não forneça resultados ótimos
De uma forma geral devemos preferir ELU > leaky ReLU > ReLU >> tanh > sigmoide
As funções ReLU, Leaky ReLU e ELU são muito melhores do que as funções sigmoidais
Quanto às ativações sigmoidais, a função TanH é significantemente melhor do que a Sigmoide
-- Sigmóide
Funciona melhor no caso de classificadores
Às vezes é evitada devido ao problema de Vanishing Gradient
-- Tanh
Às vezes é evitada devido ao problema de Vanishing Gradient
-- ReLU
Função de ativação geral, utilizada na maioria dos casos atualmente
Deve ser usada apenas nas camadas ocultas
-- Leaky ReLU
Boa para casos com neurônios deficientes nas redes

4.4 REPRESENTAÇÃO NUMÉRICA DOS DADOS
(TODO)
Explicar qual biblioteca foi utilizada, os diferentes testes e resultados.

4.5 ÍNDICES PARA ANÁLISE DA QUALIDADE DA DETECÇÃO
(TODO)
Métricas:
Loss, Accuracy model (%), Accuracy detection (%), MAPE, RMSE, Matriz de Confusão

5. RESULTADOS
(TODO)



+++ ANALISE DE DADOS

- Tamanho dos textos por tipo de notícia fake/true

- Work cloud of fake news

- Work cloud of true news

- Exemplos de stopwords

- Listar o top 25 palavras nos textos: Todos, true e false
from collections import Counter 
counter = Counter(all_words)
counter.most_common(25)
counted_words = Counter(all_words)
words = []
counts = []
for letter, count in counted_words.most_common(25):
    words.append(letter)
    counts.append(count)
colors = cm.rainbow(np.linspace(0, 1, 10))
rcParams['figure.figsize'] = 20, 10
plt.title('Top words in Text')
plt.xlabel('Count')
plt.ylabel('Words')
plt.barh(words, counts, color=colors)

- Exemplos de palavras semelhantes
w2v_model.wv.most_similar("fbi")



+++ Montagem dos modelos
Utilizaram bastante a função de ativação ReLU

- 1
A saída do LSTM bidirecional passou por três camadas densas (512, 128, 4 unidades) separadas por um dropout.
Usando a função de ativação soft-max na camada de saída, o resultado é uma classificação de postura (não relacionada, concordar, discordar ou discutir)
Adam optimizer

- 2
Utilizou ativação RELU em algumas camadas, quais não deu pra entender
Camada de saída, Sigmóide, com otimizador Adam

- 3 MODELO LSTM
Adicionei uma camada de Dropout para mitigar problemas de sobreajuste

Uma adição à configuração necessária é que uma camada LSTM antes de cada camada LSTM subsequente retorne a sequência,
o que pode ser feito definindo o parâmetro “return_sequences = True” na(s) camada(s) anterior(es).



+++ SAVE MODEL
#
# # Definir modelo
# model = Sequential()
# model.add(LSTM(...))
# # Compilar modelo
# model.compile(...)
# # Ajustar modelo
# model.fit(...)
# # Salvar modelo em arquivo
# model.save('nome_arquivo.h5')
#
# from keras.models import load_model
# # Carregar modelo de arquivo
# model = load_model('nome_arquivo.h5')



+++ ATUALIZAÇÕES:
- Removido o cronograma;
- Pesquisado um pouco mais sobre funções de ativação (ainda não foi colocado no artigo);
- Analisado o que os trabalhos correlatos utilizam de funções de ativação e métricas de desempenho;
- Adicionado o tópico Validação Cruzada;
- Adicionado o tópico Algoritmos de Treinamento e o algoritmo Adam;
- Adicionado o tópico de Representação Numérica dos Dados;
- Melhorado o tópico de Tratamento dos Dados;



+++ Dúvidas
- keras Dense é o modelo MLP?
- Dropout se encaixa como uma camada ou não seria bem isso, é algo mais simples?
- Outra coisa que achei estranho, posso misturar camadas Dense com LSTM, os exemplo de modelos LSTM que achei utilizam Dense
- O que eu achei estranho, eu posso criar um modelo keras que tem só uma camada, mas na teoria deveria existir pelo menos três, entrada, uma intermediária e saída






