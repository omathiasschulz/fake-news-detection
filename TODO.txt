
4. DESENVOLVIMENTO

4.1 DADOS DISPONÍVEIS (Pronto)

4.2 TRATAMENTO DOS DADOS (Pronto)

4.3 RECURSOS UTILIZADOS

4.3.1 Bibliotecas (Pronto)

4.3.1.1  Numpy (Pronto)
4.2.1.2  Pandas (Pronto)
4.3.1.3  Matplotlib (Pronto)
4.3.1.4  Seaborn (Pronto)
4.3.1.5  NLTK (Pronto)
4.3.1.6  Gensim (Pronto)
4.3.1.7  Keras (Pronto)

4.3.1 Funções de Ativação
(TODO) Texto quase pronto - Falta explicar as funções de ativação que serão utilizadas

Começar com a função ReLU e depois passar para outras funções de ativação no caso da ReLU não forneça resultados ótimos
De uma forma geral devemos preferir ELU > leaky ReLU > ReLU >> tanh > sigmoide
As funções ReLU, Leaky ReLU e ELU são muito melhores do que as funções sigmoidais
Quanto às ativações sigmoidais, a função TanH é significantemente melhor do que a Sigmoide
-- Sigmóide
Funciona melhor no caso de classificadores
Às vezes é evitada devido ao problema de Vanishing Gradient
-- Tanh
Às vezes é evitada devido ao problema de Vanishing Gradient
-- ReLU
Função de ativação geral, utilizada na maioria dos casos atualmente
Deve ser usada apenas nas camadas ocultas
-- Leaky ReLU
Boa para casos com neurônios deficientes nas redes

4.3.2 Validação Cruzada
(TODO)

4.3.4 Algoritmos de otimização
(TODO)

4.3.4.1 ADAM
(TODO)

4.4 REPRESENTAÇÃO NUMÉRICA DOS DADOS
(TODO)

4.5 ÍNDICES PARA ANÁLISE DA QUALIDADE DA DETECÇÃO
(TODO)
=> Exemplo RMSE, mas ainda não sei quais vou usar certo

5. RESULTADOS
(TODO)
=> LSTM => Keras 'Biblioteca de aprendizado profundo para Theano e TensorFlow'
=> LSTM => scikit-learn 'Aprendizado de máquina fácil de usar e de uso geral em Python'
