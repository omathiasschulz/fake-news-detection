
4. DESENVOLVIMENTO

4.1 DADOS DISPONÍVEIS (Pronto)

4.2 TRATAMENTO DOS DADOS (Pronto)

4.3 RECURSOS UTILIZADOS

4.3.1 Bibliotecas (Pronto)

4.3.1.1  Numpy (Pronto)
4.2.1.2  Pandas (Pronto)
4.3.1.3  Matplotlib (Pronto)
4.3.1.4  Seaborn (Pronto)
4.3.1.5  NLTK (Pronto)
4.3.1.6  Gensim (Pronto)
4.3.1.7  Keras (Pronto)

4.3.1 Funções de Ativação
(TODO) Texto quase pronto - Falta explicar as funções de ativação que serão utilizadas

Começar com a função ReLU e depois passar para outras funções de ativação no caso da ReLU não forneça resultados ótimos
De uma forma geral devemos preferir ELU > leaky ReLU > ReLU >> tanh > sigmoide
As funções ReLU, Leaky ReLU e ELU são muito melhores do que as funções sigmoidais
Quanto às ativações sigmoidais, a função TanH é significantemente melhor do que a Sigmoide
-- Sigmóide
Funciona melhor no caso de classificadores
Às vezes é evitada devido ao problema de Vanishing Gradient
-- Tanh
Às vezes é evitada devido ao problema de Vanishing Gradient
-- ReLU
Função de ativação geral, utilizada na maioria dos casos atualmente
Deve ser usada apenas nas camadas ocultas
-- Leaky ReLU
Boa para casos com neurônios deficientes nas redes

+++ Motangem dos modelos
Utilizaram bastante a função de ativaçãoReLU

- 1
A saída do LSTM bidirecional passou por três camadas densas (512, 128, 4 unidades) separadas por um dropout.
Usando a função de ativação soft-max na camada de saída, o resultado é uma classificação de postura (não relacionada, concordar, discordar ou discutir)
Adam optimizer

- 2
Utilizou ativação RELU em algumas camadas, quais não deu pra entender
Camada de saída, Sigmóide, com otimizador Adam

4.3.3 Validação Cruzada (Pronto)

4.3.4 Algoritmos de otimização (Pronto)

4.3.4.1 Adam (Pronto)

4.4 REPRESENTAÇÃO NUMÉRICA DOS DADOS
(TODO)

4.5 ÍNDICES PARA ANÁLISE DA QUALIDADE DA DETECÇÃO
(TODO)
=> Exemplo RMSE, mas ainda não sei quais vou usar certo

- Precision

- Recall
A recuperação, também conhecida como sensibilidade, é a fração de ocorrências significativas que foram recuperadas sobre a quantidade total de ocorrências relevantes.

- F1-Score
Pontuação F também pontuação F ou medida F é uma medida da precisão de um teste para classificação binária.

- Accuracy

- Matriz de Confusão
0 é negativo e 1 é positivo.
Verdadeiro Negativo (TN): A previsão foi negativa e os casos de teste também foram negativos;
Verdadeiro Positivo (TP): A previsão foi positiva e os casos de teste também foram positivos;
Falso negativo (FN): A previsão foi negativa, mas os casos de teste foram realmente positivos;
Falso Positivo (FP): A previsão foi positiva, mas os casos de teste foram realmente negativos;

5. RESULTADOS
(TODO)
=> LSTM => Keras 'Biblioteca de aprendizado profundo para Theano e TensorFlow'
=> LSTM => scikit-learn 'Aprendizado de máquina fácil de usar e de uso geral em Python'



+++ ATUALIZAÇÕES:
- Removido o cronograma;
- Pesquisado um pouco mais sobre funções de ativação (ainda não foi colocado no artigo);
- Analisado o que os trabalhos correlatos utilizam de funções de ativação e métricas de desempenho;
- Adicionado o tópico Validação Cruzada;
- Adicionado o tópico Algoritmos de Treinamento e o algoritmo Adam;
