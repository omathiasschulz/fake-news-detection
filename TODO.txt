+++ TÓPICOS QUE AINDA SERÃO COMPLEMENTADOS

4.3.1 Funções de Ativação
(TODO)
Texto quase pronto - Falta explicar as funções de ativação que serão utilizadas

Começar com a função ReLU e depois passar para outras funções de ativação no caso da ReLU não forneça resultados ótimos
De uma forma geral devemos preferir ELU > leaky ReLU > ReLU >> tanh > sigmoide
As funções ReLU, Leaky ReLU e ELU são muito melhores do que as funções sigmoidais
Quanto às ativações sigmoidais, a função TanH é significantemente melhor do que a Sigmoide
-- Sigmóide
Funciona melhor no caso de classificadores
Às vezes é evitada devido ao problema de Vanishing Gradient
-- Tanh
Às vezes é evitada devido ao problema de Vanishing Gradient
-- ReLU
Função de ativação geral, utilizada na maioria dos casos atualmente
Deve ser usada apenas nas camadas ocultas
-- Leaky ReLU
Boa para casos com neurônios deficientes nas redes

4.4 REPRESENTAÇÃO NUMÉRICA DOS DADOS
(TODO)
Explicar qual biblioteca foi utilizada, os diferentes testes e resultados.

4.5 ÍNDICES PARA ANÁLISE DA QUALIDADE DA DETECÇÃO
(TODO)
=> Exemplo RMSE, mas ainda não sei quais vou usar certo

- Precision

- Recall
A recuperação, também conhecida como sensibilidade, é a fração de ocorrências significativas que foram recuperadas sobre a quantidade total de ocorrências relevantes.

- F1-Score
Pontuação F também pontuação F ou medida F é uma medida da precisão de um teste para classificação binária.

- Accuracy

- Matriz de Confusão
0 é negativo e 1 é positivo.
Verdadeiro Negativo (TN): A previsão foi negativa e os casos de teste também foram negativos;
Verdadeiro Positivo (TP): A previsão foi positiva e os casos de teste também foram positivos;
Falso negativo (FN): A previsão foi negativa, mas os casos de teste foram realmente positivos;
Falso Positivo (FP): A previsão foi positiva, mas os casos de teste foram realmente negativos;

5. RESULTADOS
(TODO)
LSTM Keras***



+++ ATUALIZAÇÕES:
- Removido o cronograma;
- Pesquisado um pouco mais sobre funções de ativação (ainda não foi colocado no artigo);
- Analisado o que os trabalhos correlatos utilizam de funções de ativação e métricas de desempenho;
- Adicionado o tópico Validação Cruzada;
- Adicionado o tópico Algoritmos de Treinamento e o algoritmo Adam;
- Adicionado o tópico de Representação Numérica dos Dados;
- Melhorado o tópico de Tratamento dos Dados;



+++ Montagem dos modelos
Utilizaram bastante a função de ativação ReLU

- 1
A saída do LSTM bidirecional passou por três camadas densas (512, 128, 4 unidades) separadas por um dropout.
Usando a função de ativação soft-max na camada de saída, o resultado é uma classificação de postura (não relacionada, concordar, discordar ou discutir)
Adam optimizer

- 2
Utilizou ativação RELU em algumas camadas, quais não deu pra entender
Camada de saída, Sigmóide, com otimizador Adam
